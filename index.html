<!DOCTYPE html>
<html>

<head>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            background-color: #f5f5f5;
        }

        a {
            color: #4183C4;
            text-decoration: none;
        }

        p {
            line-height: 20px;
        }

        .content {
            max-width: 800px;
            margin: auto;
        }

        #abs {
            text-align: center;
        }

        #abs .descriptor {
            display: none;
        }

        #abs h1.title {
            margin: .5em 0 .5em 20px;
            font-size: x-large;
            font-weight: bold;
            line-height: 120%;
        }

        #abs .authors {
            margin: .5em 0 .5em 20px;
            font-size: medium;
            line-height: 150%;
        }

        #abs .authors a {
            font-size: medium;
        }

        #abs p {
            text-align: justify;
        }

        .bib {
            font-size: small;
        }

        .figure {
            text-align: center;
        }
    </style>
</head>
<body>
<div class="content">
    <div id="abs">
        <h1>Joint Latent Space EBM Prior Model for Multi-layer Generator</h1>
        <div class="authors"><a href="mailto:jcui7@stevens.edu">Jiali Cui</a><sup>1</sup>, <a href="mailto:ywu@stat.ucla.edu">Ying Nian Wu</a><sup>2</sup>, <a href="mailto:than6@stevens.edu">Tian Han</a><sup>1</sup>,</div>
        <div class="inst">
            <sup>1</sup> Stevens Institute of Technology, SIT<br>
            <sup>2</sup> University of California, Los Angeles, UCLA<br>
        </div>
        <h2>Abstract</h2>
        <p> This paper studies the fundamental problem of learning multi-layer generator models. The multi-layer generator model builds multiple layers of latent variables as a prior model on top of the generator, which benefits learning complex data distribution and hierarchical representations. However, such a prior model usually focuses on modeling inter-layer relations between latent variables by assuming non-informative (conditional) Gaussian distributions, which can be limited in model expressivity. To tackle this issue and learn more expressive prior models, we propose an energy-based model (EBM) on the joint latent space over all layers of latent variables with the multi-layer generator as its backbone. Such joint latent space EBM prior model captures the intra-layer contextual relations at each layer through layer-wise energy terms, and latent variables across different layers are jointly corrected. We develop a joint training scheme via maximum likelihood estimation (MLE), which involves Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior distributions of the latent variables from different layers. To ensure efficient inference and learning, we further propose a variational training scheme where an inference model is used to amortize the costly posterior MCMC sampling. Our experiments demonstrate that the learned model can be expressive in generating high-quality images and capturing hierarchical features for better outlier detection.</p>
    </div>

    <hr>
    <h2>Paper</h2>
    comming soon
<!--    The publication can be obtained <a href="https://arxiv.org/abs/2006.08205">here</a>.-->

<!--    <pre class="bib">-->
<!--@article{pang2020ebmprior,-->
<!--  title={Learning Latent Space Energy-Based Prior Model},-->
<!--  author={Pang, Bo and Han, Tian and Nijkamp, Erik and Zhu, Song-Chun and Wu, Ying Nian},-->
<!--  journal={NeurIPS},-->
<!--  year={2020}-->
<!--}-->
<!--    </pre>-->


<!--    <h2>Contributions</h2>-->
<!--    <p>(1) We propose a generator model with a latent space energy-based prior model by following the empirical Bayes philosophy.<br>-->
<!--(2) We develop the maximum likelihood learning algorithm based on MCMC sampling of the latent vector from the prior and posterior distributions.<br>-->
<!--(3) We further develop an efficient modification of MLE learning based on short-run MCMC sampling.<br>-->
<!--(4) We provide theoretical foundation for learning driven by short-run MCMC.<br>-->
<!--(5) We provide strong empirical results to corroborate the proposed method.</p>-->
<!--    <hr>-->

    <hr>
    <h2>Code</h2>
<!--    The code can be obtained <a href="https://github.com/jcui1224/hierarchical-joint-ebm">here</a>.-->
    comming soon

    <hr>

    <h2>Experiments</h2>

    <h3>Experiment 1: Joint Training</h3>
    <p>
        The proposed EBM prior model can be jointly trained with generation model (MCMC posterior) and inference model (variational learning).
        If the model is well-trained, the multi-layer EBM prior model should render expressive prior distribution leading to realistic synthesis.
        We recruit Fr√©chet Inception Distance (FID) to quantitatively evaluate the generation quality and compare with the baseline models that assume standard Gassuain prior and informative prior distribution.
    </p>
    <div class="figure">
        <img src="./figures/exp1-joint-training/cvpr2023-exp1-table.png" width="400" style="margin:10px">
    </div>

    <p>
        We further compare two posterior sampling scheme: MCMC posterior vs. Inference model.
        We observe that the MCMC posterior sampling can be more accurate but less efficient, while inference model is efficient but can be less accurate.
    </p>
    <div class="figure">
        <img src="./figures/exp1-joint-training/cvpr2023-exp1-mcmc-vs-inf.png" width="400" style="margin:10px">
    </div>


    <h3>Experiment 2: Deep Hierarchical Model</h3>
    <p>
        The proposed EBM prior model can also be applied to deep hierarchical models via two-stage training scheme, in which the posterior samples can be obtained from pre-trained inference model,
        while prior samples are obtained via <a href="https://github.com/NVlabs/VAEBM">reparametrized sampling</a> scheme.
        We consider <a href="https://github.com/NVlabs/NVAE">NVAE</a> as our backbone model for the first stage training and train our joint latent space EBM prior model in the second stage.
    </p>

    <p>The quantitative results are shown in tables</p>
    <div class="figure">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-table1.png" width="300" style="margin:10px">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-table2.png" width="300" style="margin:10px">
<!--        <p class="caption"><b>Table</b> Image synthesis with NVAE backbone model on CelebA-HQ \(256 \times 256\). Left: temperature=0.7. Right: temperature=1.0.</p>-->
    </div>

    <p>The qualitative results are shown in figures</p>
    <div class="figure">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-celebahq256_ours_0.7.png" width="300" style="margin:10px">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-celebahq256_ours_1.0.png" width="300" style="margin:10px">
        <p class="caption"><b>Figure 1:</b> Image synthesis with NVAE backbone model on CelebA-HQ (\(256 \times 256\)). Left: temperature=0.7. Right: temperature=1.0.</p>
    </div>
    <div class="figure">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-cifar10_nvae.png" width="300" style="margin:10px">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-cifar10_ours.png" width="300" style="margin:10px">
        <p class="caption"><b>Figure 2:</b> Image synthesis with NVAE backbone model on CIFAR-10 (\(32 \times 32\)). Left: NVAE. Right: ours.</p>
    </div>
    <div class="figure">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-church_nave.png" width="300" style="margin:10px">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-church_ours.png" width="300" style="margin:10px">
        <p class="caption"><b>Figure 3:</b> Image synthesis with NVAE backbone model on LSUN-Church (\(64 \times 64\)). Left: NVAE. Right: ours.</p>
    </div>

    <p>
        We further visualize the Langevin transition that starts from Gaussian prior toward our joint EBM prior distribution.
        It can be seen that the quality of synthesis improves as the Langevin progresses
    </p>
    
    <div class="figure">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-cifar10_mcmc1.png" width="300" style="margin:10px">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-cifar10_mcmc2.png" width="300" style="margin:10px">
        <p class="caption"><b>Figure 1:</b> Langevin transition on CIFAR-10.</p>
    </div>
    <div class="figure">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-church_mcmc.png" width="600" style="margin:10px">
        <p class="caption"><b>Figure 1:</b> Langevin transition on LSUN-Church.</p>
    </div>
    <div class="figure">
        <img src="./figures/exp2-nvae/cvpr2023-exp2-celeba256_mcmc.png" width="600" style="margin:10px">
        <p class="caption"><b>Figure 1:</b> Langevin transition on CelebA-HQ.</p>
    </div>
    
<!--    <div class="figure">-->
<!--        <img src="./figure/image/fid/fid_table.png" width="800" style="margin:10px">-->
<!--    </div>-->


<!--    <h3>Experiment 2: Text</h3>-->
<!--    <p> To evaluate the quality of the generated samples, we recruit Forward Perplexity (FPPL) and Reverse Perplexity (RPPL). FPPL is the perplexity of the generated samples evaluated under a language model trained with real data and measures the fluency of the synthesized sentences. RPPL is the perplexity of real data (the test data partition) computed under a language model trained with the model-generated samples. Prior work employs it to measure the distributional coverage of a learned model, \(p_\theta(x)\) in our case, since a model with a mode-collapsing issue results in a high RPPL. FPPL and RPPL are displayed in Table 2. Our model outperforms all the baselines on the two metrics, demonstrating the high fluency and diversity of the samples from our model. </p>-->
<!--    <div class="figure">-->
<!--        <img src="./figure/text/text_table.png" width="800" style="margin:10px">-->
<!--    </div>-->


<!--    <h3>Experiment 3: Analysis of latent space</h3>-->
<!--    <p> We examine the exponential tilting of the reference prior \(p_0(z)\) through Langevin samples initialized from \(p_0(z)\) with target distribution \(p_\alpha(z)\). As the reference distribution \(p_0(z)\) is in the form of an isotropic Gaussian, we expect the energy-based correction \(f_\alpha\) to tilt \(p_0\) into an irregular shape like some shallow local modes. Therefore, the trajectory of a Markov chain initialized from the reference distribution \(p_0(z)\) with well-learned target \(p_\alpha(z)\) should depict the transition towards synthesized examples of high quality while the energy fluctuates around some constant. Figure 2 depicts such transitions for CelebA, which is based on a model trained with \(K_0 = 40\) steps. The quality of synthesis improves significantly with increasing number of steps.-->
<!-- </p>-->
<!--    <div class="figure">-->
<!--        <img src="./figure/transition/image_transition.png" width="800" style="margin:10px">-->
<!--        <p class="caption"><b>Figure 2:</b> Transition of Markov chains initialized from \(p_0(z)\) towards \(\tilde{p}_{\alpha}(z)\) for \(K_0'=100\) steps. Top: Trajectory in the CelebA data-space. Bottom: Energy profile over time.</p>-->
<!--    </div>-->


<!--    <h3>Experiment 4: Anomaly detection</h3>-->
<!--    <p> If the generator and EBM are well learned, then the posterior \(p_\theta(z|x)\) would form a discriminative latent space that has separated probability densities for normal and anomalous data. Samples from such latent space can then be used as discriminative features to detect anomalies. We perform posterior sampling on the learned model to obtain the latent samples, and use the unnormalized log-posterior \(\log p_\theta(x, z)\) as our decision function. </p>-->
<!--    <div class="figure">-->
<!--        <img src="./figure/anomaly/anomaly_table.png" width="800" style="margin:10px">-->
<!--    </div>-->


<!--    <h3>Experiment 5: Scalability</h3>-->
<!--    <p> We have also explored avenues to improve training speed and found that a PyTorch extension, NVIDIA Apex, is able to improve our model training 2.5 times. We test our method with Apex training on a larger scale dataset, CelebA \( (128 \times 128 \times 3) \). The learned model is able to synthesize examples with high fidelity. </p>-->
<!--    <div class="figure">-->
<!--        <img src="./figure/image/celeba_large/celeba128_synthesis.png" width="600" style="margin:10px">-->
<!--    </div>-->


<!--    <hr>-->
<!--    <h2>Acknowledgements</h2>-->
<!--    <p>The work is supported by NSF DMS-2015577, DARPA XAI N66001-17-2-4029, ARO W911NF1810296, ONR MURI N00014-16-1-2007, and XSEDE grant ASC170063. We thank NVIDIA for the donation of Titan V GPUs.</p>-->
</div>
</body>
</html>
